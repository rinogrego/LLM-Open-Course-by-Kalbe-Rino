{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a60ddbd45d3470f8d25901717c8a07c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e062919edc242f9b7f4c630fee76cb1",
              "IPY_MODEL_48786a415e924c0b8a266394fb228906",
              "IPY_MODEL_c1d2ce414b3a4b398d8269400ca84b8b"
            ],
            "layout": "IPY_MODEL_8ffea995ed8945eaa37052a06db5f4d6"
          }
        },
        "2e062919edc242f9b7f4c630fee76cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f85b02b3ce18432bada596b59da462f7",
            "placeholder": "​",
            "style": "IPY_MODEL_977515e355e649a2a04cea8f5dce8d6f",
            "value": "model.safetensors: 100%"
          }
        },
        "48786a415e924c0b8a266394fb228906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1593f070aefa4f0193d4df4d1475e8a8",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca448cdf66c7439aac7341a823323714",
            "value": 440449768
          }
        },
        "c1d2ce414b3a4b398d8269400ca84b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_031dfbdf6fea4261b88f25cc541fffd8",
            "placeholder": "​",
            "style": "IPY_MODEL_66272d7309a1453cb94e33b6f41d1f55",
            "value": " 440M/440M [00:04&lt;00:00, 90.1MB/s]"
          }
        },
        "8ffea995ed8945eaa37052a06db5f4d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f85b02b3ce18432bada596b59da462f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "977515e355e649a2a04cea8f5dce8d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1593f070aefa4f0193d4df4d1475e8a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca448cdf66c7439aac7341a823323714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "031dfbdf6fea4261b88f25cc541fffd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66272d7309a1453cb94e33b6f41d1f55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Catalogue - Question and Answering\n",
        "\n",
        "Personality: You are an expert in Natural Language Processing. You are proficient in using huggingface transformers library and pytorch deep learning framework."
      ],
      "metadata": {
        "id": "VE9MYW-zu1Dz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain about Question and Answering in LLM, how the task works, and provides sample codes along with explanation in comment using transformers library in Python"
      ],
      "metadata": {
        "id": "hcL2T_CUu-tI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, as a fellow enthusiast in NLP and machine learning, I'd be happy to provide an overview of Question and Answering (Q&A) in Language Model (LLM) using Hugging Face's Transformers library and PyTorch.\n",
        "\n",
        "Question and Answering tasks involve providing a model with a passage of text and a question, and the model is expected to extract the relevant answer from the passage. Large Language Models (LLMs) like BERT, GPT, and their variants have shown significant success in this domain.\n",
        "\n",
        "Here's a simple example using the Hugging Face Transformers library and PyTorch:"
      ],
      "metadata": {
        "id": "dHBuOs0ovHwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "# !pip install transformers\n",
        "# !pip install torch\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the question-answering model from transformers\n",
        "qa_pipeline = pipeline('question-answering', model='bert-base-uncased', tokenizer='bert-base-uncased')\n",
        "\n",
        "# Example passage\n",
        "passage = \"Hugging Face Transformers is a powerful library for natural language processing.\"\n",
        "\n",
        "# Ask a question related to the passage\n",
        "question = \"What is Hugging Face Transformers used for?\"\n",
        "\n",
        "# Get the answer using the model\n",
        "answer = qa_pipeline(question=question, context=passage)\n",
        "\n",
        "# Print the answer\n",
        "print(answer['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWGfqVcRvRBO",
        "outputId": "c7617ef3-3801-4687-adbe-c5d2610ff4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face Transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "1. We use the `pipeline` class from the Transformers library to load a pre-trained question-answering model (in this case, BERT-base-uncased) along with its tokenizer.\n",
        "2. We define a passage and a question.\n",
        "3. We use the `qa_pipeline` to get the model's answer to the given question based on the provided passage.\n",
        "Feel free to adapt the model and passage according to your specific use case. If you have any specific questions or need further clarification, feel free to ask!"
      ],
      "metadata": {
        "id": "b1KBx8XFvjGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Theoretically, explain about Question and Answering task in NLP. What kind of data needed, which processing method is at least needed, how to structure the input X and output y for training the language model for the task, how the model learns, and what evaluation metrics needed to evaluate the model."
      ],
      "metadata": {
        "id": "KOG3TNutvoge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Question and Answering (Q&A) task in NLP, the goal is to build a model that can understand and answer questions based on a given context or passage. Here's a theoretical overview:\n",
        "\n",
        "**Data Requirements:**\n",
        "1. **Contextual Data:** You need a dataset with pairs of passages or contexts along with associated questions and their corresponding answers.\n",
        "2. **Annotations:** The dataset should be annotated with the specific span or region of the passage that contains the correct answer to each question.\n",
        "\n",
        "**Processing Methods:**\n",
        "1. **Tokenization:** Text data needs to be tokenized into smaller units, typically words or subwords, so that it can be processed by the model.\n",
        "2. **Context-Question Pairing:** Create input pairs consisting of the passage and its corresponding question. The model needs to understand the relationship between the two.\n",
        "\n",
        "**Structuring Input X and Output y:**\n",
        "1. **Input (X):**\n",
        "\n",
        "  - **Context:** The passage or document serving as the context for the question.\n",
        "  - **Question:** The input question.\n",
        "  - Concatenate or somehow combine these to create the model input.\n",
        "2. **Output (y):**\n",
        "\n",
        "  - **Answer Span:** The model should output the start and end indices of the answer span within the passage.\n",
        "\n",
        "**Model Learning:**\n",
        "1. **Pre-training:** The model is pre-trained on a large corpus using unsupervised learning, learning to predict the next word in a sequence (e.g., BERT's masked language model pre-training).\n",
        "2. **Fine-tuning:** The pre-trained model is fine-tuned on the Q&A dataset. During fine-tuning, the model learns to predict the answer span given the context and question.\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "1. **Exact Match (EM):** Measures the percentage of predicted spans that match the ground truth exactly.\n",
        "2. **F1 Score:** Evaluates the overlap between the predicted and true answer spans, considering precision and recall.\n",
        "3. **BLEU Score:** Evaluates the quality of the generated answer by comparing it to a set of reference answers.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "In summary, for Q&A in NLP, you need annotated data, a proper structure for input-output pairs, and a model capable of understanding context-question relationships. Training involves pre-training on a large dataset and fine-tuning on a Q&A-specific dataset. Evaluation metrics measure how well the model predicts answer spans compared to the ground truth."
      ],
      "metadata": {
        "id": "T9dwYY2YvrFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From your theoretical overview, elaborate each point and each subpoint to be more precise. Also give simple python code for each explanation. Give short and concise explanation by the comment inside the code."
      ],
      "metadata": {
        "id": "MX9e5el4wrse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Certainly! Let's break down each point and subpoint with concise Python code and comments:"
      ],
      "metadata": {
        "id": "RMXUnAmSwtkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Data Requirements:**"
      ],
      "metadata": {
        "id": "CNDGYMhuwwH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contextual Data\n",
        "# Dataset with passages, questions, and answers\n",
        "# Example using pandas DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "data = {'passage': [\"Hugging Face Transformers is a powerful library for NLP.\",\n",
        "                    \"BERT, GPT, and RoBERTa are popular NLP models.\"],\n",
        "        'question': [\"What is Hugging Face Transformers?\",\n",
        "                     \"Name some popular NLP models.\"],\n",
        "        'answer': [\"A powerful library for NLP.\",\n",
        "                   \"BERT, GPT, and RoBERTa.\"]}\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "Bt6dH6Tcx2iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Processing Methods:**"
      ],
      "metadata": {
        "id": "crx9QdwQwzRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Using transformers library for tokenization\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example tokenization\n",
        "tokenized_input = tokenizer(df['passage'][0], df['question'][0], return_tensors='pt')"
      ],
      "metadata": {
        "id": "dq7Od9gwx3pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Structuring Input X and Output y:**"
      ],
      "metadata": {
        "id": "1SOo7WEgw3Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Structuring Input (X)\n",
        "# Concatenating passage and question for model input\n",
        "input_X = tokenizer(df['passage'][0], df['question'][0], return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Structuring Output (y)\n",
        "# Answer span indices in the passage\n",
        "start_idx = df['passage'][0].find(df['answer'][0])\n",
        "end_idx = start_idx + len(df['answer'][0]) - 1\n",
        "output_y = {'start_positions': start_idx, 'end_positions': end_idx}"
      ],
      "metadata": {
        "id": "Cvc0N_fNx5Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## author's code\n",
        "display(df[\"passage\"][0])\n",
        "display(df[\"question\"][0])\n",
        "display(df[\"answer\"][0])\n",
        "display(input_X)\n",
        "display(start_idx)\n",
        "display(end_idx)\n",
        "display(output_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "eAw1voQQr247",
        "outputId": "624ef2a3-1fa6-4adc-8f9f-603a712bd2ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Hugging Face Transformers is a powerful library for NLP.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'What is Hugging Face Transformers?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'A powerful library for NLP.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101, 17662,  2227, 19081,  2003,  1037,  3928,  3075,  2005, 17953,\n",
              "          2361,  1012,   102,  2054,  2003, 17662,  2227, 19081,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'start_positions': -1, 'end_positions': 25}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Model Learning:**"
      ],
      "metadata": {
        "id": "RFuMDhZFw4qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-training and Fine-tuning\n",
        "# Using transformers library for pre-trained models and fine-tuning\n",
        "from transformers import BertForQuestionAnswering, AdamW\n",
        "\n",
        "# Load pre-trained model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Fine-tuning with example data\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss = model(**input_X, **output_y).loss\n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789,
          "referenced_widgets": [
            "9a60ddbd45d3470f8d25901717c8a07c",
            "2e062919edc242f9b7f4c630fee76cb1",
            "48786a415e924c0b8a266394fb228906",
            "c1d2ce414b3a4b398d8269400ca84b8b",
            "8ffea995ed8945eaa37052a06db5f4d6",
            "f85b02b3ce18432bada596b59da462f7",
            "977515e355e649a2a04cea8f5dce8d6f",
            "1593f070aefa4f0193d4df4d1475e8a8",
            "ca448cdf66c7439aac7341a823323714",
            "031dfbdf6fea4261b88f25cc541fffd8",
            "66272d7309a1453cb94e33b6f41d1f55"
          ]
        },
        "id": "gA4mQx48r5U4",
        "outputId": "172433c2-829b-4da9-dc87-db4a8e46186f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a60ddbd45d3470f8d25901717c8a07c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-33dcbe084d35>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Fine-tuning with example data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moutput_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_positions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mend_positions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m             \u001b[0;31m# If we are on multi-GPU, split add a dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1868\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_positions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1869\u001b[0m                 \u001b[0mstart_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_positions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_positions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Evaluation Metrics:**"
      ],
      "metadata": {
        "id": "TGN2glsZw7Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "# Using transformers library for evaluation\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize Q&A pipeline\n",
        "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example evaluation\n",
        "prediction = qa_pipeline(question=df['question'][0], context=df['passage'][0])\n",
        "exact_match = int(prediction['answer'] == df['answer'][0])\n",
        "f1_score = 2 * (exact_match * prediction['score']) / (exact_match + prediction['score'])"
      ],
      "metadata": {
        "id": "QS0QJ8XIuIG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These code snippets provide a practical demonstration of each step in the Q&A process, from data preparation to model training and evaluation, using the Transformers library in Python. Adjustments can be made based on the specific dataset and requirements."
      ],
      "metadata": {
        "id": "1KYu-pq6w9uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your suggestion give error during loss = model(**input_X, **output_y).loss. It gives error message: 'int' object has no attribute 'size'"
      ],
      "metadata": {
        "id": "XzE2n3ZDw_uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apologies for the oversight. It seems there was an error in the code snippet. The **`input_X`** and **`output_y`** need to be properly formatted for the model. Here's the corrected version:"
      ],
      "metadata": {
        "id": "OBG9pv1pxBuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Corrected Structuring Input X and Output y\n",
        "# Structuring Input (X)\n",
        "# Concatenating passage and question for model input\n",
        "input_X = tokenizer(df['passage'][0], df['question'][0], return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Structuring Output (y)\n",
        "# Answer span indices in the passage\n",
        "start_idx = torch.tensor([df['passage'][0].find(df['answer'][0])])\n",
        "end_idx = torch.tensor([start_idx + len(df['answer'][0]) - 1])\n",
        "output_y = {'start_positions': start_idx, 'end_positions': end_idx}\n",
        "\n",
        "# Convert start and end positions to long type\n",
        "output_y = {key: val.long() for key, val in output_y.items()}"
      ],
      "metadata": {
        "id": "konF65eBxN57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, this should properly format the input and output for the model during the training phase. Thank you for catching that mistake, and I appreciate your understanding."
      ],
      "metadata": {
        "id": "l9QclDhuxNG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End Chat"
      ],
      "metadata": {
        "id": "5w0LMLJuxj5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-training and Fine-tuning\n",
        "# Using transformers library for pre-trained models and fine-tuning\n",
        "from transformers import BertForQuestionAnswering, AdamW\n",
        "\n",
        "# Load pre-trained model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Fine-tuning with example data\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss = model(**input_X, **output_y).loss\n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quQvPZGGuJn5",
        "outputId": "e324ae7e-0456-4278-8774-5bf76a4e8e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "# Using transformers library for evaluation\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize Q&A pipeline\n",
        "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example evaluation\n",
        "prediction = qa_pipeline(question=df['question'][0], context=df['passage'][0])\n",
        "exact_match = int(prediction['answer'] == df['answer'][0])\n",
        "f1_score = 2 * (exact_match * prediction['score']) / (exact_match + prediction['score'])\n",
        "\n",
        "\n",
        "print(\"passage      :\", df['passage'][0])\n",
        "print(\"question     :\", df['question'][0])\n",
        "print(\"answer       :\", df['answer'][0])\n",
        "print(\"prediction   :\", prediction)\n",
        "print(\"exact_match  :\", exact_match)\n",
        "print(\"f1_score     :\", f1_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxvU9WpJsR1f",
        "outputId": "8a6176e1-0280-4b2d-c79b-73a5316d6046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passage      : Hugging Face Transformers is a powerful library for NLP.\n",
            "question     : What is Hugging Face Transformers?\n",
            "answer       : A powerful library for NLP.\n",
            "prediction   : {'score': 0.00835273414850235, 'start': 0, 'end': 39, 'answer': 'Hugging Face Transformers is a powerful'}\n",
            "exact_match  : 0\n",
            "f1_score     : 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FImmLYXpsjcN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}