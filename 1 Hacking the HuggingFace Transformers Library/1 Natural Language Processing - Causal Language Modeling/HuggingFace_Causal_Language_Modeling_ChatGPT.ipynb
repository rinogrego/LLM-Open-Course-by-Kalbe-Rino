{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Catalogue - Causal Language Modeling\n",
        "\n",
        "Personality: You are an expert in Natural Language Processing. You are proficient in using huggingface transformers library and pytorch deep learning framework."
      ],
      "metadata": {
        "id": "bkbH43tO1WmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain about Causal Language Modeling in LLM, how the task works, and provides sample codes along with explanation in comment using transformers library in Python"
      ],
      "metadata": {
        "id": "CHK1XjNn1pA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Causal Language Modeling (CLM) is a type of language modeling task where the goal is to predict the next word in a sequence given the context of the preceding words. It's often implemented using autoregressive models, where each prediction influences the next one.\n",
        "\n",
        "In the context of transformers and Hugging Face's Transformers library, this is typically done using models like GPT (Generative Pre-trained Transformer). Here's a simple example using the library:"
      ],
      "metadata": {
        "id": "CDBXZkCy1rd8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7nIJr2myYBo",
        "outputId": "6333b4db-eead-4ac0-ba8f-037ccc15bb57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Causal language modeling is a technique used in natural language processing to predict the meaning of words. It is a method that is used to predict the meaning of words in natural language processing. It is a method that is used to predict the meaning of words\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # or any other GPT-2 variant\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Input text for context\n",
        "# context = \"Causal language modeling is a technique used in natural language processing to...\" ## ChatGPT example suggestion gives weird output\n",
        "context = \"Causal language modeling is a technique used in natural language processing to\" ## author's input\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
        "\n",
        "# Generate predictions for the next word\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the `GPT2LMHeadModel` is loaded along with its tokenizer. The `encode` method is used to convert the input text into tokenized form, and the `generate` method predicts the next word(s) based on the given context.\n",
        "\n",
        "Remember, the context is crucial for accurate predictions. Adjust the `max_length` parameter in the `generate` method to control the length of the generated sequence.\n",
        "\n",
        "Feel free to adapt this code to your specific use case and let me know if you have any questions or need further clarification!"
      ],
      "metadata": {
        "id": "WfVO_mFw1uaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Theoretically, explain about Causal Language Modeling task in NLP. What kind of data needed, which processing method is at least needed, how to structure the input X and output y for training the language model for the task, how the model learns, and what evaluation metrics needed to evaluate the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tzzcr25K11Of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Causal Language Modeling (CLM) is a task in Natural Language Processing (NLP) where the goal is to predict the next word in a sequence given the context of the preceding words. This task falls under the umbrella of language modeling, and it's particularly implemented using autoregressive models.\n",
        "\n",
        "**Theoretical Explanation**:\n",
        "1. **Data Requirements:**\n",
        "\n",
        "  - **Text Corpus:** You need a large text corpus to train the language model. This corpus should be representative of the language you want the model to understand and generate.\n",
        "\n",
        "2. **Preprocessing:**\n",
        "\n",
        "  - **Tokenization:** The text data needs to be tokenized, breaking it down into smaller units like words or subwords. This step is essential for creating a numerical representation of the text that the model can work with.\n",
        "\n",
        "3. **Data Structuring:**\n",
        "\n",
        "  - **Sequential Input-Output Pairs:** For CLM, you structure the data into sequential input-output pairs, where the input is the context of the preceding words, and the output is the next word to be predicted. Essentially, you're creating a dataset of (X, y) pairs, where X is the input sequence, and y is the target word.\n",
        "\n",
        "4. **Model Architecture:**\n",
        "\n",
        "  - **Autoregressive Models:** Models like GPT (Generative Pre-trained Transformer) are commonly used for CLM. These models are autoregressive, meaning they predict the next word based on the preceding words in the sequence.\n",
        "\n",
        "5. **Learning Process:**\n",
        "\n",
        "  - **Training Objective:** The model is trained to minimize the negative log likelihood of predicting the correct next word. During training, the model conditions on the context and predicts the next word, and the parameters are adjusted based on the error between the predicted and actual words.\n",
        "\n",
        "6. **Evaluation Metrics:**\n",
        "\n",
        "  - **Perplexity:** Perplexity is a common metric for language modeling tasks, including CLM. It measures how well the model predicts a sequence and is essentially a measure of how surprised the model is by the next word. Lower perplexity indicates better performance.\n",
        "\n",
        "  **Summary:**\n",
        "- **Data:** Large text corpus.\n",
        "- **Processing:** Tokenization.\n",
        "- **Input X and Output y:** Sequential input-output pairs.\n",
        "- **Model:** Autoregressive models like GPT.\n",
        "- **Learning:** Minimizing negative log likelihood during training.\n",
        "- **Evaluation Metrics:** Perplexity.\n",
        "\n",
        "Understanding and implementing these concepts will help you effectively approach the Causal Language Modeling task in NLP. If you have any specific questions or need further clarification, feel free to ask!"
      ],
      "metadata": {
        "id": "1vJRc0IV13vH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From your theoretical overview, elaborate each point and each subpoint to be more precise. Also give simple python code for each explanation. Give short and concise explanation by the comment inside the code."
      ],
      "metadata": {
        "id": "YUY44OkU2i6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Certainly! Let's break down each point with concise explanations and provide simple Python code with comments."
      ],
      "metadata": {
        "id": "axJ9KHE92kjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Point 1: Data Requirements\n",
        "# Subpoint: Text Corpus\n",
        "# Example: Assuming you have a text corpus in a file named 'corpus.txt'\n",
        "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
        "    text_corpus = file.read()\n",
        "\n",
        "# Point 2: Preprocessing\n",
        "# Subpoint: Tokenization\n",
        "# Example: Using the Hugging Face Transformers library for tokenization\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenized_corpus = tokenizer.tokenize(text_corpus)\n",
        "\n",
        "# Point 3: Data Structuring\n",
        "# Subpoint: Sequential Input-Output Pairs\n",
        "# Example: Creating (X, y) pairs for training\n",
        "input_sequence = tokenized_corpus[:-1]\n",
        "target_word = tokenized_corpus[1:]\n",
        "\n",
        "# Point 4: Model Architecture\n",
        "# Subpoint: Autoregressive Models\n",
        "# Example: Using Hugging Face Transformers library to load a pre-trained GPT model\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Point 5: Learning Process\n",
        "# Subpoint: Training Objective\n",
        "# Example: Assuming you have DataLoader and optimizer set up\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Convert tokens to tensors\n",
        "input_ids = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "labels = tokenizer.convert_tokens_to_ids(target_word)\n",
        "input_ids_tensor = torch.tensor([input_ids])\n",
        "labels_tensor = torch.tensor([labels])\n",
        "\n",
        "# Forward pass\n",
        "outputs = model(input_ids_tensor, labels=labels_tensor)\n",
        "loss = outputs.loss\n",
        "\n",
        "# Backward pass and optimization\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "# Point 6: Evaluation Metrics\n",
        "# Subpoint: Perplexity\n",
        "# Example: Calculating perplexity using cross-entropy loss\n",
        "import math\n",
        "\n",
        "# Assuming you have model predictions and true labels\n",
        "predictions = model(input_ids_tensor).logits\n",
        "true_labels = labels_tensor\n",
        "\n",
        "# Calculate cross-entropy loss\n",
        "cross_entropy_loss = CrossEntropyLoss()(predictions.view(-1, model.config.vocab_size), true_labels.view(-1))\n",
        "\n",
        "# Calculate perplexity\n",
        "perplexity = math.exp(cross_entropy_loss.item())\n",
        "\n",
        "# Print perplexity\n",
        "print(f\"Perplexity: {perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP2rTycVzpMo",
        "outputId": "bb445340-266a-4fcc-c1cd-f45237087f90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 30.87808034961313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These examples provide a concise overview and implementation for each point and subpoint in the theoretical explanation of Causal Language Modeling. Feel free to adapt the code to your specific needs or ask if you have any questions!"
      ],
      "metadata": {
        "id": "iQvrwiNk2npI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elaborate the type or structure of language data that should be stored in corpus.txt and give example."
      ],
      "metadata": {
        "id": "uF59JYJx2o81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `corpus.txt` file should contain a diverse and representative set of text data that is relevant to the language you want your model to understand and generate. The corpus serves as the training data for the language model. Here's an example structure for the `corpus.txt` file:"
      ],
      "metadata": {
        "id": "9gx1z5L22qbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile corpus.txt\n",
        "Title: The Adventures of Sherlock Holmes\n",
        "\n",
        "Chapter 1: A Scandal in Bohemia\n",
        "To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name.\n",
        "\n",
        "Chapter 2: The Red-Headed League\n",
        "I had called upon my friend, Mr. Sherlock Holmes, one day in the autumn of last year...\n",
        "\n",
        "...\n",
        "\n",
        "News Article: Breakthrough in Renewable Energy\n",
        "Scientists have announced a groundbreaking discovery in renewable energy...\n",
        "\n",
        "...\n",
        "\n",
        "Conversational Dialogue:\n",
        "Person A: How was your day?\n",
        "Person B: It was good, thanks. I finished reading that book you recommended...\n",
        "\n",
        "...\n",
        "\n",
        "Technical Documentation:\n",
        "Function Name: calculate_average\n",
        "Parameters: list_of_numbers\n",
        "Returns: float\n",
        "Description: This function calculates the average of a list of numbers...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1jPQjOa0166",
        "outputId": "84f2c66d-57c2-4194-d425-72fae7bae768"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing corpus.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the `corpus.txt` file includes a mix of narrative text (Sherlock Holmes story), news articles, conversational dialogue, and technical documentation. This diversity helps the model learn a broad range of language patterns and contexts.\n",
        "\n",
        "Make sure your corpus reflects the language and domains you expect the model to be proficient in. Including a variety of writing styles, topics, and formats will enhance the model's ability to generate coherent and contextually appropriate text in different situations."
      ],
      "metadata": {
        "id": "CBwDO42B2u-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End Chat"
      ],
      "metadata": {
        "id": "BmMVd7wK20wl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YAhrV0e31LPO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}